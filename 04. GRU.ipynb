{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'transformers'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1589554c5118>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtorchtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mbuild_vocab_from_iterator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mAutoModelForPreTraining\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#import config as cfg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from transformers import AutoTokenizer, AutoModelForPreTraining\n",
    "\n",
    "#import config as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_model = AutoModelForPreTraining.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "tokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = {'Historia': 0, 'Administracao': 1, 'Geografia': 2, 'Biologia': 3, \n",
    "          'Literatura': 4, 'Artes': 5, 'Matematica': 6}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('../data/processed/train_final.csv')\n",
    "x_test = pd.read_csv('../data/processed/test.csv')\n",
    "\n",
    "x_test.drop(['titulo', 'genero'], axis=1, inplace=True)\n",
    "\n",
    "# x_train['alt_title'] = x_train['alt_title'].map(lambda title: \\\n",
    "#                                                 tokenizer.encode_plus(title, \n",
    "#                                                                       add_special_tokens=True,\n",
    "#                                                                       padding='longest'))\n",
    "# x_test['alt_title'] = x_test['alt_title'].map(lambda title: \\\n",
    "#                                               tokenizer.encode_plus(title, \n",
    "#                                                                     add_special_tokens=True,\n",
    "#                                                                     padding='longest'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataframe: pd.DataFrame, tokenizer, max_len: int = None) -> object:\n",
    "        \n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer \n",
    "        self.max_len = max_len \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        textos = self.data.loc[index, 'alt_title'], \n",
    "        labels = self.data.loc[index, 'label']\n",
    "\n",
    "        encoding = tokenizer(textos, \n",
    "                             max_length=self.max_len,\n",
    "                             padding='max_length',\n",
    "                             truncation=True,\n",
    "                             return_tensors='pt')\n",
    "        \n",
    "        inputs = encoding['input_ids']\n",
    "        tkn_type = encoding['token_type_ids']\n",
    "        att_mask = encoding['attention_mask']\n",
    "\n",
    "        return inputs, tkn_type, att_mask, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(x_train, tokenizer=tokenizer, max_len=32)\n",
    "test_dataset = MyDataset(x_test, tokenizer=tokenizer, max_len=32)\n",
    "\n",
    "del x_train, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "class CustomBERTModel(torch.nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "        \n",
    "        # Load the pre-trained BERT model and tokenizer\n",
    "        # self.bert = pt_bert_model \n",
    "        self.bert = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "        \n",
    "        for p in self.bert.embeddings.parameters(): \n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fc = torch.nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Forward pass through the BERT model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "        # Extract the last hidden state of the token `[CLS]` for classification task\t\n",
    "        pooled_output  = outputs.pooler_output\n",
    "\n",
    "        # Pass the pooled output through the classification layer\n",
    "        logits = self.fc(pooled_output)\n",
    "\n",
    "        # Apply softmax activation function to the classification output\n",
    "        probabilities = self.softmax(logits)\n",
    "\n",
    "        return probabilities, logits\n",
    "\n",
    "model = CustomBERTModel(num_classes=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertModel\n",
    "\n",
    "class CustomBERTModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomBERTModel, self).__init__()\n",
    "\n",
    "        # Load the pre-trained BERT model and tokenizer\n",
    "        self.bert = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
    "\n",
    "        # Optionally, you can choose to freeze the embeddings\n",
    "        for p in self.bert.embeddings.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.fc1 = nn.Linear(self.bert.config.hidden_size, 256)  # Additional linear layer\n",
    "        self.gelu = nn.GELU()\n",
    "        self.fc2 = nn.Linear(256, 128)  # Additional linear layer\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_tp_ids):\n",
    "        # Forward pass through the BERT model\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_tp_ids)\n",
    "\n",
    "        # Extract the last hidden state of the token [CLS] for classification task\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        # Apply the additional linear layer with ReLU activation\n",
    "        x = self.fc1(pooled_output)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.fc4(x)\n",
    "        x = self.gelu(x)\n",
    "\n",
    "        # Pass the transformed output through the classification layer\n",
    "        logits = self.fc5(x)\n",
    "        print(logits)\n",
    "        return logits  # Softmax is applied outside the model during training\n",
    "\n",
    "model = CustomBERTModel(num_classes=7)\n",
    "# model = CustomBERTModel(num_classes=7)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from torch import nn\n",
    "\n",
    "# model = CustomBERTModel(num_classes=7, pt_bert_model=emb_model)\n",
    "model = CustomBERTModel(num_classes=7)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(10): \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (inputs, tkn_type, att_mask, labels) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        probabilities, logits = model(inputs[0], att_mask)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Epoch: {epoch}, Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)\n",
    "\n",
    "# Initialize variables to keep track of training statistics\n",
    "total_loss = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(10):\n",
    "    model.train()  # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for batch_idx, (inputs, tkn_type, att_mask, labels) in enumerate(train_loader):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "    \n",
    "        logits = model(inputs.squeeze(1), att_mask.squeeze(1), tkn_type.squeeze(1))\n",
    "        # print(outputs)\n",
    "        # Calculate the loss\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "\n",
    "        # Backward pass and update the model parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy for the current batch\n",
    "        predicted = torch.softmax(logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        # Print training statistics for the epoch\n",
    "        accuracy = 100 * correct / total\n",
    "        print(f'batch_idx [{batch_idx + 1}/{len(train_loader)}], Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%', end='\\r')\n",
    "\n",
    "    # Print training statistics for the epoch\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Epoch [{epoch + 1}/10], Loss: {total_loss / len(train_loader):.4f}, Accuracy: {accuracy:.2f}%')\n",
    "\n",
    "# Final accuracy after training\n",
    "print('Final Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
